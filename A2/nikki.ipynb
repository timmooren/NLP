{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\nikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### whole brown corpus ###\n",
    "corpus = [word for word in brown.words() if any(c.isalpha() for c in word)]\n",
    "\n",
    "# sorted list for whole corpus\n",
    "fdist = nltk.FreqDist([w.lower() for w in corpus])\n",
    "sorted_whole = sorted(fdist, key=fdist.get, reverse=True)\n",
    "# for word in sorted_whole[:10]:\n",
    "#     print(word, fdist[word])\n",
    "\n",
    "### sorted lists for two brown categories ###\n",
    "lore = brown.words(categories='lore')\n",
    "lore = [word for word in lore if any(c.isalpha() for c in word)]\n",
    "adventure = brown.words(categories='adventure')\n",
    "adventure = [word for word in adventure if any(c.isalpha() for c in word)]\n",
    "\n",
    "loredist = nltk.FreqDist([w.lower() for w in lore])\n",
    "advdist = nltk.FreqDist([w.lower() for w in adventure])\n",
    "\n",
    "sorted_lore = sorted(loredist, key=loredist.get, reverse=True)\n",
    "sorted_adv = sorted(advdist, key=advdist.get, reverse=True)\n",
    "\n",
    "# for word in sorted_lore[:10]:\n",
    "#     print(word, loredist[word])\n",
    "\n",
    "# for word in sorted_adv[:10]:\n",
    "#     print(word, advdist[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 156580), ('IN', 136387), ('DT', 114735), ('JJ', 80467), ('NNP', 76598), ('NNS', 54215), ('VBD', 47175), ('RB', 45062), ('PRP', 44082), ('CC', 37957), ('VB', 34480), ('VBN', 27168), ('TO', 26158), ('VBZ', 21594), ('VBG', 17967), ('PRP$', 17306), ('VBP', 16516), ('MD', 12459), ('CD', 6879), ('WDT', 5633), ('WRB', 4385), ('WP', 4259), ('RP', 4173), ('JJR', 3055), ('EX', 2335), ('JJS', 1956), ('RBR', 1755), ('NNPS', 1709), ('PDT', 952), ('RBS', 661), ('WP$', 250), ('FW', 172), ('UH', 23), ('POS', 10), ('$', 3), (\"''\", 3)]\n"
     ]
    }
   ],
   "source": [
    "# number of tokens\n",
    "whole = brown.words()\n",
    "print(f\"nr of tokens, with punctuation: {len(whole)}\")  \n",
    "print(f\"nr of tokens, without punctuation: {len(corpus)}\")\n",
    "\n",
    "# number of types and words\n",
    "print(f\"nr of types: {len(set(whole))}\") # print(f\" {}\")\n",
    "print(f\"nr of words: {len(corpus)}\")\n",
    "\n",
    "# average number of words per sentence; average word length\n",
    "print(f\"average nr of words per sentence: {len(corpus)/len(brown.sents())}\")\n",
    "count = 0\n",
    "for word in corpus:\n",
    "    count += len(word)\n",
    "print(f\"average word length: {count / len(corpus)}\")\n",
    "\n",
    "# POS tags\n",
    "tagged = nltk.pos_tag(corpus)\n",
    "# print(tagged[:10])\n",
    "# print(brown.tagged_words(tagset='universal')[:10])\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in tagged)\n",
    "print(f\"10 most frequent POS-tags: {tag_fd.most_common()[:10]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE SLOW, DOES NOT WORK'\n",
    "\n",
    "# plt.plot(list(fdist.keys()), list(fdist.values()), label='corpus', color='black')  # plot the frequency curve for the corpus\n",
    "# plt.xlabel('Rank')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II: UNIGRAM MODEL\n",
    "1. Creating the word_to_index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "812\n",
      "813\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "NLP A2: N-Gram Language Models\n",
    "\n",
    "@author: Klinton Bicknell, Harry Eldridge, Nathan Schneider, Lucia Donatelli, Alexander Koller\n",
    "\n",
    "DO NOT SHARE/DISTRIBUTE SOLUTIONS WITHOUT THE INSTRUCTOR'S PERMISSION\n",
    "\"\"\"\n",
    "\n",
    "word_index_dict = {}\n",
    "\n",
    "# TODO: read brown_vocab_100.txt into word_index_dict\n",
    "\n",
    "f = open(\"brown_vocab_100.txt\", \"r\")\n",
    "i = 0\n",
    "for x in f:\n",
    "  word_index_dict[x.rstrip().lower()] = i\n",
    "  i += 1\n",
    "\n",
    "# for item in word_index_dict.items():\n",
    "#   print(item)\n",
    "\n",
    "# TODO: write word_index_dict to word_to_index_100.txt\n",
    "\n",
    "wf = open('word_to_index_100.txt','w')\n",
    "for item in word_index_dict.items():\n",
    "    string = str(item[0]) + ' ' + str(item[1])\n",
    "    wf.write(string + '\\n')\n",
    "wf.close()\n",
    "\n",
    "\n",
    "print(word_index_dict['all'])\n",
    "print(word_index_dict['resolution'])\n",
    "print(len(word_index_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Building a MLE unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "NLP A2: N-Gram Language Models\n",
    "\n",
    "@author: Klinton Bicknell, Harry Eldridge, Nathan Schneider, Lucia Donatelli, Alexander Koller\n",
    "\n",
    "DO NOT SHARE/DISTRIBUTE SOLUTIONS WITHOUT THE INSTRUCTOR'S PERMISSION\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from generate import GENERATE\n",
    "\n",
    "\n",
    "vocab = open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "f = open(\"brown_vocab_100.txt\", \"r\")\n",
    "i = 0\n",
    "for x in f:\n",
    "  word_index_dict[x.rstrip()] = i\n",
    "  i += 1\n",
    "\n",
    "# get sentences\n",
    "f = open(\"brown_100.txt\")\n",
    "\n",
    "# zeroes array\n",
    "counts = np.zeros(len(word_index_dict))\n",
    "\n",
    "# iterate through file and update counts\n",
    "for x in f:\n",
    "    words = x.split()\n",
    "    for word in words:\n",
    "        index = word_index_dict[word.lower()]\n",
    "        counts[index] += 1\n",
    "f.close()\n",
    "print(counts)\n",
    "\n",
    "# normalize and writeout counts. \n",
    "probs = counts / np.sum(counts)\n",
    "\n",
    "wf = open('unigram_probs.txt','w')\n",
    "for p in probs:\n",
    "    wf.write(str(p) + '\\n')\n",
    "wf.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Bigram Models\n",
    "\n",
    "3. Building an MLE bigram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "NLP A2: N-Gram Language Models\n",
    "\n",
    "@author: Klinton Bicknell, Harry Eldridge, Nathan Schneider, Lucia Donatelli, Alexander Koller\n",
    "\n",
    "DO NOT SHARE/DISTRIBUTE SOLUTIONS WITHOUT THE INSTRUCTOR'S PERMISSION\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from generate import GENERATE\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "vocab = open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "f = open(\"brown_vocab_100.txt\", \"r\")\n",
    "i = 0\n",
    "for x in f:\n",
    "  word_index_dict[x.rstrip().lower()] = i\n",
    "  i += 1\n",
    "\n",
    "\n",
    "# get sentences\n",
    "f = open(\"brown_100.txt\")\n",
    "\n",
    "# zeroes array\n",
    "counts = np.zeros((len(word_index_dict), len(word_index_dict)))\n",
    "\n",
    "# update counts for word pairs\n",
    "previous = '<s>'\n",
    "for x in f:\n",
    "    words = x.split()\n",
    "    for word in words:\n",
    "        index_previous = word_index_dict[previous.lower()]\n",
    "        index_current = word_index_dict[word.lower()]\n",
    "        counts[index_previous][index_current] +=1\n",
    "        previous = word\n",
    "f.close()\n",
    "\n",
    "# normalize counts\n",
    "probs = normalize(counts, norm='l1', axis=1)\n",
    "print(probs[word_index_dict['all'], word_index_dict['the']])\n",
    "print(probs[word_index_dict['the'], word_index_dict['jury']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Add-α smoothing the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01336573511543135\n",
      "0.05520438263801095\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "NLP A2: N-Gram Language Models\n",
    "\n",
    "@author: Klinton Bicknell, Harry Eldridge, Nathan Schneider, Lucia Donatelli, Alexander Koller\n",
    "\n",
    "DO NOT SHARE/DISTRIBUTE SOLUTIONS WITHOUT THE INSTRUCTOR'S PERMISSION\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from generate import GENERATE\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "vocab = open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "f = open(\"brown_vocab_100.txt\", \"r\")\n",
    "i = 0\n",
    "for x in f:\n",
    "  word_index_dict[x.rstrip().lower()] = i\n",
    "  i += 1\n",
    "\n",
    "\n",
    "# get sentences\n",
    "f = open(\"brown_100.txt\")\n",
    "\n",
    "# zeroes array\n",
    "counts = np.zeros((len(word_index_dict), len(word_index_dict)))\n",
    "counts += 0.1\n",
    "# update counts for word pairs\n",
    "previous = '<s>'\n",
    "for x in f:\n",
    "    words = x.split()\n",
    "    for word in words:\n",
    "        index_previous = word_index_dict[previous.lower()]\n",
    "        index_current = word_index_dict[word.lower()]\n",
    "        counts[index_previous][index_current] +=1\n",
    "        previous = word\n",
    "f.close()\n",
    "\n",
    "# normalize counts\n",
    "probs = normalize(counts, norm='l1', axis=1)\n",
    "print(probs[word_index_dict['all'], word_index_dict['the']])\n",
    "print(probs[word_index_dict['the'], word_index_dict['jury']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
